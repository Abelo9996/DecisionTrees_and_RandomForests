{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import io\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import random\n",
    "\n",
    "# A code snippet to help you save your results into a kaggle accepted csv. COPIED FROM HW1 STARTER CODE.\n",
    "def results_to_csv(y_test):\n",
    "    y_test = y_test.astype(int)\n",
    "    df = pd.DataFrame({'Category': y_test})\n",
    "    df.index += 1  # Ensures that the index starts at 1.\n",
    "    df.to_csv('submission.csv', index_label='Id')\n",
    "\n",
    "random.seed(189)\n",
    "np.random.seed(189)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE FOLLOWING CELL IS FOR LOADING THE TITANIC AND SPAM DATASETS INTO A 2D ARRAY.\n",
    "\n",
    "datasets_data = {}\n",
    "\n",
    "def load(path,mat=False,train=False):\n",
    "    if mat:\n",
    "        return io.loadmat(path)\n",
    "    else:\n",
    "        d = {}\n",
    "        data = np.genfromtxt(path, delimiter=',', dtype=None,encoding=None)\n",
    "        d[\"field_names\"] = data[0]\n",
    "        if train:\n",
    "            d[\"training_labels\"] = data[1:,0]\n",
    "        arr = np.array([])\n",
    "        for i in range(data.shape[0]):\n",
    "            if train:\n",
    "                if i > 0:\n",
    "                    arr = np.append(arr,data[i,1:])\n",
    "            else:\n",
    "                if i > 0:\n",
    "                    arr = np.append(arr,data[i,])\n",
    "        if train:\n",
    "            d[\"training_data\"] = arr.reshape(i,len(data[0])-1)\n",
    "        else:\n",
    "            d[\"test_data\"] = arr.reshape(i,len(data[0]))\n",
    "        return d\n",
    "\n",
    "datasets_data[\"spam\"] = load(\"datasets/spam_data/spam_data.mat\",mat=True)\n",
    "datasets_data[\"titanic_train\"] = load(\"datasets/titanic/titanic_training.csv\",train=True)\n",
    "datasets_data[\"titanic_test\"] = load(\"datasets/titanic/titanic_testing_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOMIZE THE DATASETS\n",
    "\n",
    "p = np.random.permutation(len(datasets_data[\"spam\"][\"training_data\"]))\n",
    "spam_train_data = datasets_data[\"spam\"][\"training_data\"][p]\n",
    "spam_train_labels = datasets_data[\"spam\"][\"training_labels\"][p]\n",
    "spam_test_data = datasets_data[\"spam\"][\"test_data\"]\n",
    "p = np.random.permutation(len(datasets_data[\"titanic_train\"][\"training_data\"]))\n",
    "titanic_train_data = datasets_data[\"titanic_train\"][\"training_data\"][p]\n",
    "titanic_train_labels = datasets_data[\"titanic_train\"][\"training_labels\"][p]\n",
    "titanic_test_data = datasets_data[\"titanic_test\"][\"test_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPUTE VALUES FOR MISSING VALUES IN TITANIC LABELS (FOR TRAINING SET) AND DATA (FOR BOTH TEST AND TRAINING SETS) WITH THE MODE VALUE. \n",
    "\n",
    "for j in range(len(titanic_train_data[0])):\n",
    "    mode = stats.mode([i for i in titanic_train_data[:,j] if i != \"\"])[0][0]\n",
    "    arr = titanic_train_data[:,j]\n",
    "    for i,c in enumerate(arr):\n",
    "        if c == \"\":\n",
    "            arr[i] = mode\n",
    "    titanic_train_data[:,j] = arr\n",
    "\n",
    "for j in range(len(titanic_test_data[0])):\n",
    "    mode = stats.mode([i for i in titanic_test_data[:,j] if i != \"\"])[0][0]\n",
    "    arr = titanic_test_data[:,j]\n",
    "    for i,c in enumerate(arr):\n",
    "        if c == \"\":\n",
    "            arr[i] = mode\n",
    "    titanic_test_data[:,j] = arr\n",
    "\n",
    "mode = stats.mode([i for i in titanic_train_labels if i != \"\"])[0][0]\n",
    "for i,c in enumerate(titanic_train_labels):\n",
    "    if c == \"\":\n",
    "        titanic_train_labels[i] = mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE HOT ENCODE PCLASS, SEX, AND EMBARKED FIELDS.\n",
    "\n",
    "categorical_features = [0,0,6] # COLUMN INDEXES FOR THE FIELDS MENTIONED ABOVE (AFTER REMOVING SEQUENTIALLY).\n",
    "for j in categorical_features:\n",
    "    arr = list(set(titanic_train_data[:,j]))\n",
    "    for item in arr:\n",
    "        im_arr = (titanic_train_data[:,j] == item).astype('int')\n",
    "        titanic_train_data = np.append(titanic_train_data,np.reshape(im_arr,(titanic_train_data.shape[0],1)),axis=1)\n",
    "    titanic_train_data = np.delete(titanic_train_data,j,axis=1)\n",
    "\n",
    "for j in categorical_features:\n",
    "    arr = list(set(titanic_test_data[:,j]))\n",
    "    for item in arr:\n",
    "        im_arr = (titanic_test_data[:,j] == item).astype('int')\n",
    "        titanic_test_data = np.append(titanic_test_data,np.reshape(im_arr,(titanic_test_data.shape[0],1)),axis=1)\n",
    "    titanic_test_data = np.delete(titanic_test_data,j,axis=1)\n",
    "\n",
    "\n",
    "# REMOVE ORIGINAL COLUMNS AFTER BEING DONE WITH ONE-HOT ENCODING.\n",
    "titanic_test_data = np.delete(titanic_test_data,3,axis=1)\n",
    "titanic_test_data = np.delete(titanic_test_data,4,axis=1)\n",
    "titanic_train_data = np.delete(titanic_train_data,3,axis=1)\n",
    "titanic_train_data = np.delete(titanic_train_data,4,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-5\n",
    "\n",
    "def w(x):\n",
    "    return np.int(hash(x)) % 1000\n",
    "\n",
    "h = np.vectorize(w)\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5, feature_labels=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.features = feature_labels\n",
    "        self.split_rule = [None,None]\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.label = None\n",
    "\n",
    "    @staticmethod\n",
    "    def information_gain(X, y, thresh):\n",
    "        return DecisionTree.entropy(y) - (len(X[np.where(X>=thresh)[0]])*DecisionTree.entropy(y[np.where(X>=thresh)[0]])+len(X[np.where(X<thresh)[0]])*DecisionTree.entropy(y[np.where(X<thresh)[0]]))/len(X)\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(y):\n",
    "        entropy = 0\n",
    "        for i in np.unique(y):\n",
    "            fraction = np.count_nonzero(y==i)/len(y)\n",
    "            entropy -= fraction*np.log2(fraction)\n",
    "        return entropy\n",
    "\n",
    "    def split(self, X, y, idx, thresh):\n",
    "        left_idx = np.where(X[:,idx]<thresh)[0]\n",
    "        right_idx = np.where(X[:,idx]>=thresh)[0]\n",
    "        return X[left_idx], y[left_idx], X[right_idx], y[right_idx]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.max_depth == 0:\n",
    "            self.label = stats.mode(y).mode[0]\n",
    "        else:\n",
    "            width = X.shape[1]\n",
    "            thresholds = [np.linspace(min(X[:,i])+eps,max(X[:,i])-eps, num=10) for i in range(width)]\n",
    "            ig_arr = []\n",
    "            for i in range(width):\n",
    "                ig_arr.append([self.information_gain(X[:,i],y,threshold) for threshold in thresholds[i]])\n",
    "            ig_arr = np.array(ig_arr)\n",
    "            self.split_rule[0], idx = np.unravel_index(np.argmax(ig_arr), ig_arr.shape)\n",
    "            thresholds = np.array(thresholds)\n",
    "            self.split_rule[1] = thresholds[self.split_rule[0],idx]\n",
    "            left_X, left_y, right_X, right_y = self.split(X, y, self.split_rule[0], self.split_rule[1])\n",
    "            if left_X.shape[0] == 0 or right_X.shape[0] == 0:\n",
    "                self.max_depth = 0\n",
    "                self.label = stats.mode(y).mode[0]\n",
    "            else:\n",
    "                self.left = DecisionTree(self.max_depth-1, self.features)\n",
    "                self.right = DecisionTree(self.max_depth-1, self.features)\n",
    "                self.left.fit(left_X, left_y)\n",
    "                self.right.fit(right_X, right_y)\n",
    "\n",
    "    def predict(self, X, split = False):\n",
    "        if self.max_depth == 0:\n",
    "            return np.array([1]*X.shape[0])*self.label\n",
    "        else:\n",
    "            left_idx = np.where(X[:,self.split_rule[0]]<self.split_rule[1])[0]\n",
    "            if split and len(X) == 1:\n",
    "                    if len(X[left_idx]) == 1:\n",
    "                        print(\"(\"+str(self.features[self.split_rule[0]])+\")\"+\" < \"+str(self.split_rule[1]))\n",
    "                    else:\n",
    "                        print(\"(\"+str(self.features[self.split_rule[0]])+\")\"+\" >= \"+str(self.split_rule[1]))\n",
    "            ret_pred = np.array([0]*X.shape[0])\n",
    "            ret_pred[left_idx] = self.left.predict(X[left_idx],split)\n",
    "            right_idx = np.where(X[:,self.split_rule[0]]>=self.split_rule[1])[0]\n",
    "            ret_pred[right_idx] = self.right.predict(X[right_idx],split)\n",
    "            return ret_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggedTrees(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, max_depth=3, n=20, cut=100):\n",
    "        self.max_depth = max_depth\n",
    "        self.n = n\n",
    "        self.cut = cut\n",
    "        self.decision_trees = [\n",
    "            DecisionTree(max_depth, None, None)\n",
    "            for i in range(self.n)\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for model in self.decision_trees:\n",
    "            idx = np.random.randint(0, X.shape[0], self.cut)\n",
    "            model.fit(X[idx], y[idx])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array(np.round(np.mean([model.predict(X) for model in self.decision_trees],axis=0))).astype(np.bool_)\n",
    "\n",
    "class RandomForest(BaggedTrees):\n",
    "    def __init__(self, max_depth=3, n=20, cut=100):\n",
    "        self.max_depth = max_depth\n",
    "        self.n = n\n",
    "        self.cut = cut\n",
    "        self.decision_trees = [\n",
    "            DecisionTree(max_depth, None)\n",
    "            for i in range(self.n)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For categorical variables, I used one-hot encoding on the Pclass, Sex, and Embarked columns to turn these categorical variables into numeric ones of the same weight. Moreover, since one-hot encoding Ticket and Cabin would not produce any useful information due to its uniqueness, I decided to remove it. Lastly, regarding the missing data, I used the mode value of each column to fill in the missing information required for the training datasets.\n",
    "\n",
    "2. I used the max depth as my stopping criterion, where I would return the most common class available as the label when reached.\n",
    "\n",
    "3. I used a list of decision trees where from some random subset of the features, every node would choose a splitting feature that would then be used to deduce the returned prediction via majority vote in the total amount of trees implemented.\n",
    "\n",
    "4. I did not do anything regarding the speedup.\n",
    "\n",
    "5. I really haven't implemented anything else that has been cool, sadly :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle Username: Abel Yagubyan\n",
    "\n",
    "Spam score: 78.623%\n",
    "\n",
    "Titanic score: 80.645%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\",\n",
    "            \"creative\", \"height\", \"featured\", \"differ\", \"width\", \"other\",\n",
    "            \"energy\", \"business\", \"message\", \"volumes\", \"revision\", \"path\",\n",
    "            \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "            \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\",\n",
    "            \"square_bracket\", \"ampersand\"]\n",
    "idx = int(0.8*spam_train_data.shape[0])\n",
    "model = DecisionTree(max_depth=5, feature_labels=features)\n",
    "model.fit(spam_train_data[:idx], spam_train_labels[:idx])\n",
    "print(\"Training Accuracy for spam in decision tree: \"+str(accuracy_score(model.predict(spam_train_data[:idx]),spam_train_labels[:idx])))\n",
    "print(\"Validation Accuracy for spam in decision tree: \" + str(accuracy_score(model.predict(spam_train_data[idx:]),spam_train_labels[idx:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForest(max_depth=5, n=50, cut=len(spam_train_data))\n",
    "model.fit(spam_train_data[:idx], spam_train_labels[:idx])\n",
    "print(\"Training Accuracy for spam in random forest: \"+str(accuracy_score(model.predict(spam_train_data[:idx]),spam_train_labels[:idx])))\n",
    "print(\"Validation Accuracy for spam in random forest: \" + str(accuracy_score(model.predict(spam_train_data[idx:]),spam_train_labels[idx:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train_data = titanic_train_data.astype(float)\n",
    "titanic_test_data = titanic_test_data.astype(float)\n",
    "titanic_train_labels = titanic_train_labels.astype(np.int64)\n",
    "features = ['x'+str(i) for i in range(12)]\n",
    "idx = int(0.8*titanic_train_data.shape[0])\n",
    "model = DecisionTree(max_depth=5, feature_labels=features)\n",
    "model.fit(titanic_train_data[:idx], titanic_train_labels[:idx])\n",
    "print(\"Training Accuracy for titanic in decision tree: \"+str(accuracy_score(model.predict(titanic_train_data[:idx]),titanic_train_labels[:idx])))\n",
    "print(\"Validation Accuracy for titanic in decision tree: \" + str(accuracy_score(model.predict(titanic_train_data[idx:]),titanic_train_labels[idx:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForest(max_depth=5, n=50, cut=len(titanic_train_data))\n",
    "model.fit(titanic_train_data[:idx], titanic_train_labels[:idx])\n",
    "print(\"Training Accuracy for titanic in random forest: \"+str(accuracy_score(model.predict(titanic_train_data[:idx]),titanic_train_labels[:idx])))\n",
    "print(\"Validation Accuracy for titanic in random forest: \" + str(accuracy_score(model.predict(titanic_train_data[idx:]),titanic_train_labels[idx:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle submission for titanic\n",
    "model = RandomForest(max_depth=10, n=100, cut=len(titanic_train_data))\n",
    "model.fit(titanic_train_data, titanic_train_labels)\n",
    "results_to_csv(model.predict(titanic_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle submission for spam\n",
    "model = RandomForest(max_depth=15, n=100,cut=len(spam_train_data))\n",
    "model.fit(spam_train_data, spam_train_labels)\n",
    "results_to_csv(model.predict(spam_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subpart 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\",\n",
    "            \"creative\", \"height\", \"featured\", \"differ\", \"width\", \"other\",\n",
    "            \"energy\", \"business\", \"message\", \"volumes\", \"revision\", \"path\",\n",
    "            \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "            \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\",\n",
    "            \"square_bracket\", \"ampersand\"]\n",
    "model = DecisionTree(max_depth=25, feature_labels=features)\n",
    "model.fit(spam_train_data, spam_train_labels)\n",
    "predictions = model.predict(spam_train_data)\n",
    "\n",
    "ham_point = spam_train_data[(predictions == 0).nonzero()[0][0]]\n",
    "spam_point = spam_train_data[(predictions == 1).nonzero()[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.array([ham_point]), split=True)\n",
    "print(\"Therefore this email was ham.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.array([spam_point]), split=True)\n",
    "print(\"Therefore this email was spam.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subpart 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [\"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\",\n",
    "            \"creative\", \"height\", \"featured\", \"differ\", \"width\", \"other\",\n",
    "            \"energy\", \"business\", \"message\", \"volumes\", \"revision\", \"path\",\n",
    "            \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "            \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\",\n",
    "            \"square_bracket\", \"ampersand\"]\n",
    "idx = int(0.8*spam_train_data.shape[0])\n",
    "max_depths = [i for i in range(1,41)]\n",
    "accuracies = []\n",
    "\n",
    "for i in max_depths:\n",
    "    model = DecisionTree(max_depth=i, feature_labels=features)\n",
    "    model.fit(spam_train_data[:idx], spam_train_labels[:idx])\n",
    "    accuracies.append(accuracy_score(model.predict(spam_train_data[idx:]),spam_train_labels[idx:]))\n",
    "\n",
    "plt.plot(max_depths,accuracies,'r-')\n",
    "plt.xlabel(\"Max batch size\")\n",
    "plt.ylabel(\"Validation accuracy\")\n",
    "plt.title(\"Max batch size vs Validation accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest validation accuracy was at a depth of approximately 25 at 81%, however it appears that the general accuracy seems to be almost equivalent starting from a depth of 10 and onwards, hence due to the randomness of the input data, 25 is not exactly the optimal for all datasets (although it is for our particular dataset when shuffled with the particular seed of 189)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\",\n",
    "            \"creative\", \"height\", \"featured\", \"differ\", \"width\", \"other\",\n",
    "            \"energy\", \"business\", \"message\", \"volumes\", \"revision\", \"path\",\n",
    "            \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "            \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\",\n",
    "            \"square_bracket\", \"ampersand\"]\n",
    "model = DecisionTree(max_depth=3, feature_labels=features)\n",
    "model.fit(spam_train_data, spam_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
